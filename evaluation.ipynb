{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b0c3cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mar-sangineto/Documents/sorbonne/cours/deepL/projet/.venv/lib/python3.14/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from datasets import Dataset, Features, Value\n",
    "from datasets import Image as HFImage\n",
    "from transformers import AutoImageProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337caf8c",
   "metadata": {},
   "source": [
    "#### to import local funtions\n",
    "\n",
    "ATENTION: You may have to change the path so that this can run on your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6e8061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mar-sangineto/Documents/sorbonne/cours/deepL/projet/OADino'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks on which folder the notebook is initially running\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b38f0fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mar-sangineto/Documents/sorbonne/cours/deepL/projet/OADino'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change according to where are you running so that the final path contains oadino\n",
    "\n",
    "PROJECT_FOLDER_PATH = os.getcwd() # initial path + relative path to OADino\n",
    "PROJECT_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00e6c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PROJECT_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1620bb",
   "metadata": {},
   "source": [
    "Internal imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03e3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from oadino.models import OADinoModel, OADinoPreProcessor, ConvVAE16\n",
    "from oadino.models import OADinoModel, OADinoPreProcessor, ConvVAE16\n",
    "from oadino.training import get_preprocessed_data, vae_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a4862",
   "metadata": {},
   "source": [
    "#### Settup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf6b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "# elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "#     device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fc2a5",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586d5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading CLEVR dataset avaliable on https://cs.stanford.edu/people/jcjohns/clevr/\n",
    "\n",
    "\n",
    "# # loading CLEVRtex dataset avaliable on https://www.robots.ox.ac.uk/~vgg/data/clevrtex/#downloads\n",
    "\n",
    "# # loading Stanford dataset avaliable on https://huggingface.co/datasets/tanganke/stanford_cars\n",
    "# # Load the dataset in a tabular format with image URLs and metadata\n",
    "# cars_dataset = load_dataset(\"tanganke/stanford_cars\")\n",
    "\n",
    "# # Access the training set directly\n",
    "# cars_train_set = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1cfdbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from training_loop_initial_testing import create_hf_dataset\n",
    "\n",
    "def create_hf_dataset(image_dir, maxsize=-1):\n",
    "    image_paths = sorted(Path(image_dir).glob(\"*.png\"))\n",
    "\n",
    "    # Create dataset dict\n",
    "    data_dict = {\n",
    "        \"image\": [str(p) for p in image_paths[:maxsize]],\n",
    "        \"filename\": [p.name for p in image_paths[:maxsize]],\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        data_dict,\n",
    "        features=Features(\n",
    "            {\n",
    "                \"image\": HFImage(),\n",
    "                \"filename\": Value(\"string\"),\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def transform_batch(batch):\n",
    "    batch[\"image\"] = [transform(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0611495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe11a56",
   "metadata": {},
   "source": [
    "CLEVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66c18474",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../data/\"\n",
    "\n",
    "train_data_path = Path(DATA_PATH + \"/CLEVR_v1.0/images/train\")\n",
    "test_data_path = Path(DATA_PATH + \"/CLEVR_v1.0/images/test\")\n",
    "\n",
    "train_dataset = create_hf_dataset(train_data_path, maxsize=4096)\n",
    "test_dataset = create_hf_dataset(test_data_path, maxsize=4096)\n",
    "\n",
    "train_dataset = train_dataset.with_transform(transform_batch)\n",
    "train_dataset_name = \"CLEVR_train_4K_224\"\n",
    "test_dataset_name  = f\"{train_dataset_name}_test\"\n",
    "test_dataset = test_dataset.with_transform(transform_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b22c2a86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(DATA_PATH + \"/CLEVR_v1.0/images/train\").exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a92e55a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Path(test_data_path).exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e2652bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#train png: 70000 first: ../data/CLEVR_v1.0/images/train/CLEVR_train_000000.png\n",
      "#test  png: 15000 first: ../data/CLEVR_v1.0/images/test/CLEVR_test_000000.png\n"
     ]
    }
   ],
   "source": [
    "train_pngs = sorted(train_data_path.glob(\"*.png\"))\n",
    "test_pngs  = sorted(test_data_path.glob(\"*.png\"))\n",
    "\n",
    "print(\"#train png:\", len(train_pngs), \"first:\", train_pngs[0] if train_pngs else None)\n",
    "print(\"#test  png:\", len(test_pngs),  \"first:\", test_pngs[0] if test_pngs else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a144b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba2992",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c173c000",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d6749173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `BitImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. \n",
      "Loading weights: 100%|██████████| 223/223 [00:00<00:00, 1634.06it/s, Materializing param=layernorm.weight]                                 \n"
     ]
    }
   ],
   "source": [
    "## Loading Backbone Models\n",
    "hf_cache = Path(\"../data/\")\n",
    "\n",
    "dino_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"facebook/dinov2-small\", cache_dir=hf_cache\n",
    ")\n",
    "dino_model = AutoModel.from_pretrained(\"facebook/dinov2-base\", cache_dir=hf_cache).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ece63490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor = OADinoPreProcessor(dino_processor, dino_model)\n",
    "vae = ConvVAE16()\n",
    "model = OADinoModel(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6a97e",
   "metadata": {},
   "source": [
    "## Preprocessing OADino\n",
    "\n",
    "visualizing the segmentations and patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6bde322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected preprocess folder: ../data/CLEVR_train_4K_224_test/facebook_dinov2-base\n",
      "Contains: []\n"
     ]
    }
   ],
   "source": [
    "model_folder = pre_processor.backbone.config.name_or_path.replace(\"/\", \"_\")\n",
    "savedir = Path(hf_cache) / test_dataset_name / model_folder\n",
    "print(\"Expected preprocess folder:\", savedir)\n",
    "print(\"Contains:\", list(savedir.glob(\"*\"))[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e05f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "model_folder = pre_processor.backbone.config.name_or_path.replace(\"/\", \"_\")\n",
    "\n",
    "train_savedir = Path(hf_cache) / train_dataset_name / model_folder\n",
    "test_savedir  = Path(hf_cache) / f\"{train_dataset_name}_test\" / model_folder\n",
    "\n",
    "def delete_if_empty(savedir):\n",
    "    meta = savedir / \"metadata.json\"\n",
    "    if meta.exists():\n",
    "        m = json.loads(meta.read_text())\n",
    "        print(savedir, \"actual_samples:\", m.get(\"actual_samples\"))\n",
    "        if m.get(\"actual_samples\", 0) == 0:\n",
    "            print(\"Deleting bad cache:\", savedir)\n",
    "            shutil.rmtree(savedir)\n",
    "\n",
    "delete_if_empty(train_savedir)\n",
    "delete_if_empty(test_savedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c8f19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feats: torch.Size([4, 768])\n",
      "patches: torch.Size([4, 256, 3, 14, 14])\n",
      "masks: torch.Size([4, 256]) torch.bool\n",
      "foreground per image: tensor([113, 132, 148, 119])\n"
     ]
    }
   ],
   "source": [
    "debug_loader = DataLoader(train_dataset, batch_size=4, shuffle=False)\n",
    "batch = next(iter(debug_loader))\n",
    "\n",
    "with torch.no_grad():\n",
    "    feats, patches, masks = pre_processor.get_global_features_and_patches(batch[\"image\"], pca_q=None, pca_niter=2)\n",
    "\n",
    "print(\"feats:\", feats.shape)\n",
    "print(\"patches:\", patches.shape)\n",
    "print(\"masks:\", masks.shape, masks.dtype)\n",
    "print(\"foreground per image:\", masks.sum(dim=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "be76cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found directory ../data/CLEVR_train_4K_224/facebook_dinov2-base but could not load dataset\n",
      "Metadata file not found at ../data/CLEVR_train_4K_224/facebook_dinov2-base/metadata.json. Make sure the dataset was finalized properly.\n",
      "Data will be reprocessed\n",
      "Processing 4096 samples...\n",
      "Saving to: ../data/CLEVR_train_4K_224/facebook_dinov2-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [15:27<00:00, 14.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing dataset...\n",
      "Dataset saved to ../data/CLEVR_train_4K_224/facebook_dinov2-base\n",
      "Total samples processed: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_dataset = get_preprocessed_data(\n",
    "    dataset = train_dataset,\n",
    "    dataset_name = train_dataset_name,\n",
    "    image_size=224,\n",
    "    preprocessor=pre_processor,\n",
    "    base_dir=hf_cache,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f99881d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 4096 samples...\n",
      "Saving to: ../data/CLEVR_train_4K_224_test/facebook_dinov2-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 64/64 [16:35<00:00, 15.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing dataset...\n",
      "Dataset saved to ../data/CLEVR_train_4K_224_test/facebook_dinov2-base\n",
      "Total samples processed: 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_test_dataset = get_preprocessed_data(\n",
    "    dataset=test_dataset,\n",
    "    dataset_name=test_dataset_name,\n",
    "    image_size=224,\n",
    "    preprocessor=pre_processor,\n",
    "    base_dir=hf_cache,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a898169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(preprocessed_train_dataset): 4096\n",
      "len(preprocessed_test_dataset): 4096\n"
     ]
    }
   ],
   "source": [
    "print(\"len(preprocessed_train_dataset):\", len(preprocessed_train_dataset))\n",
    "print(\"len(preprocessed_test_dataset):\", len(preprocessed_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c00951ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(preprocessed_test_dataset, batch_size=64, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28814fc",
   "metadata": {},
   "source": [
    "### From the training..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63abe",
   "metadata": {},
   "source": [
    "#### Training configutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4b8b8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name = CLEVR_train_4K_224\n",
      "model_name = OADinoModel\n",
      "num_epochs = 10\n",
      "learning_rate = 0.001\n",
      "train_batch_size = 64\n",
      "test_batch_size = 64\n",
      "loss_beta = 0.0001\n",
      "image_size = 224\n",
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "config_path = PROJECT_FOLDER_PATH + \"/runs/CLEVR_train_4K_224_20260202_210355/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "for configuration in config:\n",
    "    print(configuration, \"=\", config[configuration])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568730a",
   "metadata": {},
   "source": [
    "### loading model\n",
    "\n",
    "visualizing the final trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f5145f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = \"./runs/CLEVR_train_4K_224_20260202_210355/checkpoints/\"\n",
    "checkpoints_dir = Path(checkpoints_dir)\n",
    "best_checkpoint_path = checkpoints_dir / \"best_model.pt\"\n",
    "final_checkpoint_path = checkpoints_dir / \"final_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b69aa4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "final_checkpoint = torch.load(final_checkpoint_path, map_location=device)\n",
    "\n",
    "best_model = OADinoModel(ConvVAE16()).to(device)\n",
    "final_model = OADinoModel(ConvVAE16()).to(device)\n",
    "\n",
    "\n",
    "best_model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
    "final_model.load_state_dict(final_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7f5ce476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OADinoModel(\n",
       "  (vae): ConvVAE16(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (mean_layer): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "    (logvar_layer): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=1024, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Unflatten(dim=1, unflattened_size=(256, 2, 2))\n",
       "      (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (10): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (transform): Resize(size=16, interpolation=bilinear, max_size=None, antialias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a488b33",
   "metadata": {},
   "source": [
    "# eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a52f1d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEST : {'loss': 3.9315133206546307, 'recon': 3.9315128438174725, 'kl': 0.003606314021453727}\n",
      "FINAL: {'loss': 3.931513302028179, 'recon': 3.931512825191021, 'kl': 0.003606314021453727}\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_model(model, loader, device, loss_beta=1e-4):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_recon = 0.0\n",
    "    total_kl = 0.0\n",
    "    n_batches = 0\n",
    "\n",
    "    vae_in = model.vae.input_size\n",
    "    resize_transform = Resize((vae_in, vae_in))\n",
    "\n",
    "    for batch in loader:\n",
    "        patches = batch[\"patches\"].to(device)             # (B, P, 3, ps, ps)\n",
    "        masks   = batch[\"masks\"].to(device).bool()        # (B, P)\n",
    "        flat_patches = resize_transform(patches.flatten(0, 1))  # (B*P, 3, vae_in, vae_in)\n",
    "        flat_masks   = masks.flatten()                         # (B*P,)\n",
    "\n",
    "        # Forward: only masked patches go through VAE\n",
    "        x_hat, mean, logvar = model.encode_decode_object_patches(flat_patches, flat_masks)\n",
    "\n",
    "        # Loss is computed against the masked originals\n",
    "        loss, recon, kl = vae_loss(\n",
    "            flat_patches[flat_masks],\n",
    "            x_hat, mean, logvar,\n",
    "            loss_beta,\n",
    "            return_components=True,\n",
    "        )\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kl += kl.item()\n",
    "        n_batches += 1\n",
    "\n",
    "    return {\n",
    "        \"loss\": total_loss / n_batches,\n",
    "        \"recon\": total_recon / n_batches,\n",
    "        \"kl\": total_kl / n_batches,\n",
    "    }\n",
    "\n",
    "loss_beta = config.get(\"loss_beta\", 1e-4)  # from loaded config.json\n",
    "best_metrics  = evaluate_model(best_model,  test_loader, device, loss_beta=loss_beta)\n",
    "final_metrics = evaluate_model(final_model, test_loader, device, loss_beta=loss_beta)\n",
    "\n",
    "print(\"BEST :\", best_metrics)\n",
    "print(\"FINAL:\", final_metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ef60ee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def show_reconstructions(model, dataset, device, n_show=8, seed=0):\n",
    "    model.eval()\n",
    "    g = torch.Generator().manual_seed(seed)\n",
    "\n",
    "    idx = torch.randint(0, len(dataset), (1,), generator=g).item()\n",
    "    sample = dataset[idx]\n",
    "    patches = sample[\"patches\"].to(device)      # (P, 3, ps, ps)\n",
    "    masks   = sample[\"masks\"].to(device).bool() # (P,)\n",
    "\n",
    "    # pick some foreground patches\n",
    "    fg_idx = torch.where(masks)[0]\n",
    "    if len(fg_idx) == 0:\n",
    "        print(\"No foreground patches in this sample.\")\n",
    "        return\n",
    "\n",
    "    fg_idx = fg_idx[:n_show]\n",
    "    x = patches[fg_idx]  # (n_show, 3, ps, ps)\n",
    "\n",
    "    # run through model (expects flat inputs)\n",
    "    vae_in = model.vae.input_size\n",
    "    resize_transform = Resize((vae_in, vae_in))\n",
    "    x_in = resize_transform(x)\n",
    "\n",
    "    x_hat, _, _ = model.vae(x_in)\n",
    "\n",
    "    # plot\n",
    "    n = x.shape[0]\n",
    "    fig, axes = plt.subplots(2, n, figsize=(2*n, 4))\n",
    "    for i in range(n):\n",
    "        axes[0, i].imshow(x[i].permute(1,2,0).detach().cpu().clamp(0,1))\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[0, i].set_title(\"orig\")\n",
    "\n",
    "        axes[1, i].imshow(x_hat[i].permute(1,2,0).detach().cpu().clamp(0,1))\n",
    "        axes[1, i].axis(\"off\")\n",
    "        axes[1, i].set_title(\"recon\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "show_reconstructions(best_model, preprocessed_test_dataset, device, n_show=8, seed=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4d597305",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "The shape of the mask [64, 256] at index 0 does not match the shape of the indexed tensor [8192, 32] at index 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 52\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# Build features once\u001b[39;00m\n\u001b[32m     51\u001b[39m feat_loader = DataLoader(preprocessed_test_dataset, batch_size=\u001b[32m64\u001b[39m, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m per_img_feats, pooled = \u001b[43mcompute_oadino_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve\u001b[39m(query_idx, topk=\u001b[32m10\u001b[39m, shortlist=\u001b[32m200\u001b[39m):\n\u001b[32m     55\u001b[39m     q_pool = pooled[query_idx:query_idx+\u001b[32m1\u001b[39m]          \u001b[38;5;66;03m# (1, d)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/sorbonne/cours/deepL/projet/.venv/lib/python3.14/site-packages/torch/utils/_contextlib.py:124\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    122\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-context-manager]\u001b[39;00m\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mcompute_oadino_features\u001b[39m\u001b[34m(model, loader, device)\u001b[39m\n\u001b[32m     21\u001b[39m _, mean, _ = model.encode_decode_object_patches(flat_patches, flat_masks)  \u001b[38;5;66;03m# mean is (n_masked, nl)\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# combine global+local per image (list of tensors)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m feats_list = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mglobal_feats\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fi \u001b[38;5;129;01min\u001b[39;00m feats_list:\n\u001b[32m     26\u001b[39m     per_image.append(fi.detach().cpu())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/sorbonne/cours/deepL/projet/OADino/oadino/models.py:303\u001b[39m, in \u001b[36mOADinoModel.get_features\u001b[39m\u001b[34m(self, global_features, object_patches, mask)\u001b[39m\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_features\u001b[39m(\n\u001b[32m    297\u001b[39m     \u001b[38;5;28mself\u001b[39m, global_features, object_patches, mask\n\u001b[32m    298\u001b[39m ) -> Sequence[torch.tensor]:\n\u001b[32m    299\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create OADino features from global_features (dino on masked input) and patches\u001b[39;00m\n\u001b[32m    300\u001b[39m \u001b[33;03m    Notes:\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m    - Some reviews mention using other segmentation methods (like using UNets), we could add implementations ?\u001b[39;00m\n\u001b[32m    302\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m303\u001b[39m     masked_object_patches = \u001b[43mobject_patches\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    304\u001b[39m     masked_object_patches = \u001b[38;5;28mself\u001b[39m.transform(masked_object_patches)\n\u001b[32m    305\u001b[39m     object_features, _ = \u001b[38;5;28mself\u001b[39m.vae.encode(masked_object_patches)\n",
      "\u001b[31mIndexError\u001b[39m: The shape of the mask [64, 256] at index 0 does not match the shape of the indexed tensor [8192, 32] at index 0"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def compute_oadino_features(model, loader, device):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      per_image_feats: list[Tensor] length N, each Tensor is (n_fg_patches, dim)\n",
    "      pooled_feats: Tensor (N, dim) L2-normalized mean pooling (for fast coarse search)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    per_image = []\n",
    "    pooled = []\n",
    "\n",
    "    for batch in loader:\n",
    "        global_feats = batch[\"features\"].to(device)        # (B, ng)\n",
    "        patches      = batch[\"patches\"].to(device)         # (B, P, 3, ps, ps)\n",
    "        masks        = batch[\"masks\"].to(device).bool()    # (B, P)\n",
    "\n",
    "        flat_patches = patches.flatten(0, 1)               # (B*P, 3, ps, ps)\n",
    "        flat_masks   = masks.flatten()                     # (B*P,)\n",
    "\n",
    "        # local object features for masked patches only\n",
    "        _, mean, _ = model.encode_decode_object_patches(flat_patches, flat_masks)  # mean is (n_masked, nl)\n",
    "        # combine global+local per image (list of tensors)\n",
    "        feats_list = model.get_features(global_feats, mean, masks)\n",
    "\n",
    "        for fi in feats_list:\n",
    "            per_image.append(fi.detach().cpu())\n",
    "            if fi.numel() == 0:\n",
    "                pooled.append(torch.zeros((fi.shape[-1],), dtype=torch.float32))\n",
    "            else:\n",
    "                pooled.append(fi.mean(dim=0).float())\n",
    "\n",
    "    pooled = torch.stack(pooled, dim=0)\n",
    "    pooled = F.normalize(pooled, dim=1)\n",
    "    return per_image, pooled\n",
    "\n",
    "@torch.no_grad()\n",
    "def patch_similarity(query_feats, cand_feats):\n",
    "    \"\"\"\n",
    "    query_feats: (nq, d)\n",
    "    cand_feats : (nc, d)\n",
    "    returns scalar similarity\n",
    "    \"\"\"\n",
    "    if query_feats.numel() == 0 or cand_feats.numel() == 0:\n",
    "        return -1e9\n",
    "    q = F.normalize(query_feats, dim=1)\n",
    "    c = F.normalize(cand_feats, dim=1)\n",
    "    sim = q @ c.T                      # (nq, nc)\n",
    "    return sim.max(dim=1).values.mean().item()\n",
    "\n",
    "# Build features once\n",
    "feat_loader = DataLoader(preprocessed_test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
    "per_img_feats, pooled = compute_oadino_features(best_model, feat_loader, device)\n",
    "\n",
    "def retrieve(query_idx, topk=10, shortlist=200):\n",
    "    q_pool = pooled[query_idx:query_idx+1]          # (1, d)\n",
    "    coarse = (q_pool @ pooled.T).squeeze(0)         # (N,)\n",
    "    topL = torch.topk(coarse, k=min(shortlist, len(coarse))).indices.tolist()\n",
    "\n",
    "    q_feats = per_img_feats[query_idx]\n",
    "    scored = []\n",
    "    for j in topL:\n",
    "        s = patch_similarity(q_feats, per_img_feats[j])\n",
    "        scored.append((s, j))\n",
    "    scored.sort(reverse=True)\n",
    "    return scored[:topk]\n",
    "\n",
    "# Example\n",
    "query_idx = 0\n",
    "results = retrieve(query_idx, topk=8, shortlist=200)\n",
    "print(\"Top results:\", results[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c7f78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11046297",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
