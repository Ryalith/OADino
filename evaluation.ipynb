{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b0c3cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import Resize\n",
    "from datasets import Dataset, Features, Value\n",
    "from datasets import Image as HFImage\n",
    "from transformers import AutoImageProcessor, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337caf8c",
   "metadata": {},
   "source": [
    "#### to import local funtions\n",
    "\n",
    "ATENTION: You may have to change the path so that this can run on your device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da6e8061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mar-sangineto/Documents/sorbonne/cours/deepL/projet/OADino'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checks on which folder the notebook is initially running\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f0fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/mar-sangineto/Documents/sorbonne/cours/deepL/projet/OADino'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change according to where are you running so that the final path contains oadino\n",
    "\n",
    "PROJECT_FOLDER_PATH = os.getcwd() # initial path + relative path to OADino\n",
    "PROJECT_FOLDER_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00e6c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(PROJECT_FOLDER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1620bb",
   "metadata": {},
   "source": [
    "Internal imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f03e3865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from oadino.models import OADinoModel, OADinoPreProcessor, ConvVAE16\n",
    "from oadino.models import OADinoModel, OADinoPreProcessor, ConvVAE16\n",
    "from oadino.training import get_preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684a4862",
   "metadata": {},
   "source": [
    "#### Settup device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf6b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414fc2a5",
   "metadata": {},
   "source": [
    "### Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "586d5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loading CLEVR dataset avaliable on https://cs.stanford.edu/people/jcjohns/clevr/\n",
    "\n",
    "\n",
    "# # loading CLEVRtex dataset avaliable on https://www.robots.ox.ac.uk/~vgg/data/clevrtex/#downloads\n",
    "\n",
    "# # loading Stanford dataset avaliable on https://huggingface.co/datasets/tanganke/stanford_cars\n",
    "# # Load the dataset in a tabular format with image URLs and metadata\n",
    "# cars_dataset = load_dataset(\"tanganke/stanford_cars\")\n",
    "\n",
    "# # Access the training set directly\n",
    "# cars_train_set = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cfdbde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from training_loop_initial_testing import create_hf_dataset\n",
    "\n",
    "def create_hf_dataset(image_dir, maxsize=-1):\n",
    "    image_paths = sorted(Path(image_dir).glob(\"*.png\"))\n",
    "\n",
    "    # Create dataset dict\n",
    "    data_dict = {\n",
    "        \"image\": [str(p) for p in image_paths[:maxsize]],\n",
    "        \"filename\": [p.name for p in image_paths[:maxsize]],\n",
    "    }\n",
    "\n",
    "    dataset = Dataset.from_dict(\n",
    "        data_dict,\n",
    "        features=Features(\n",
    "            {\n",
    "                \"image\": HFImage(),\n",
    "                \"filename\": Value(\"string\"),\n",
    "            }\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def transform_batch(batch):\n",
    "    batch[\"image\"] = [transform(img.convert(\"RGB\")) for img in batch[\"image\"]]\n",
    "    return batch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0611495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe11a56",
   "metadata": {},
   "source": [
    "CLEVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66c18474",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/\"\n",
    "\n",
    "train_dataset = create_hf_dataset(DATA_PATH + \"/CLEVR_v1.0/images/train\", maxsize=4096)\n",
    "test_dataset = create_hf_dataset(DATA_PATH + \"CLEVR_v1.0/images/test\", maxsize=4096)\n",
    "\n",
    "train_dataset = train_dataset.with_transform(transform_batch)\n",
    "train_dataset_name = \"CLEVR_train_4K_224\"\n",
    "test_dataset = test_dataset.with_transform(transform_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a144b6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ba2992",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0da321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c173c000",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6749173",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
      "Loading weights: 100%|██████████| 223/223 [00:00<00:00, 1031.67it/s, Materializing param=layernorm.weight]                                 \n"
     ]
    }
   ],
   "source": [
    "## Loading Backbone Models\n",
    "hf_cache = Path(\"../data/\")\n",
    "\n",
    "dino_processor = AutoImageProcessor.from_pretrained(\n",
    "    \"facebook/dinov2-small\", cache_dir=hf_cache\n",
    ")\n",
    "dino_model = AutoModel.from_pretrained(\"facebook/dinov2-base\", cache_dir=hf_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece63490",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_processor = OADinoPreProcessor(dino_processor, dino_model)\n",
    "vae = ConvVAE16()\n",
    "model = OADinoModel(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c6a97e",
   "metadata": {},
   "source": [
    "## Preprocessing OADino\n",
    "\n",
    "visualizing the segmentations and patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "be76cd59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 0 samples...\n",
      "Saving to: ../data/CLEVR_train_4K_224/facebook_dinov2-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finalizing dataset...\n",
      "Dataset saved to ../data/CLEVR_train_4K_224/facebook_dinov2-base\n",
      "Total samples processed: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "preprocessed_train_dataset = get_preprocessed_data(\n",
    "    dataset = train_dataset,\n",
    "    dataset_name = train_dataset_name,\n",
    "    image_size=224,\n",
    "    preprocessor=pre_processor,\n",
    "    base_dir=hf_cache,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28814fc",
   "metadata": {},
   "source": [
    "### From the training..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a63abe",
   "metadata": {},
   "source": [
    "#### Training configutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b8b8af1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_name = CLEVR_train_4K_224\n",
      "model_name = OADinoModel\n",
      "num_epochs = 10\n",
      "learning_rate = 0.001\n",
      "train_batch_size = 64\n",
      "test_batch_size = 64\n",
      "loss_beta = 0.0001\n",
      "image_size = 224\n",
      "device = cpu\n"
     ]
    }
   ],
   "source": [
    "config_path = PROJECT_FOLDER_PATH + \"/runs/CLEVR_train_4K_224_20260202_210355/config.json\"\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = json.load(f)\n",
    "    \n",
    "for configuration in config:\n",
    "    print(configuration, \"=\", config[configuration])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568730a",
   "metadata": {},
   "source": [
    "### loading model\n",
    "\n",
    "visualizing the final trained VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f5145f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints_dir = \"./runs/CLEVR_train_4K_224_20260202_210355/checkpoints/\"\n",
    "checkpoints_dir = Path(checkpoints_dir)\n",
    "best_checkpoint_path = checkpoints_dir / \"best_model.pt\"\n",
    "final_checkpoint_path = checkpoints_dir / \"final_model.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b69aa4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_checkpoint = torch.load(best_checkpoint_path, map_location=device)\n",
    "final_checkpoint = torch.load(final_checkpoint_path, map_location=device)\n",
    "\n",
    "best_model = model\n",
    "final_model = model\n",
    "\n",
    "\n",
    "best_model.load_state_dict(best_checkpoint[\"model_state_dict\"])\n",
    "final_model.load_state_dict(final_checkpoint[\"model_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f5ce476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OADinoModel(\n",
       "  (vae): ConvVAE16(\n",
       "    (encoder): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (2): ReLU()\n",
       "      (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU()\n",
       "      (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU()\n",
       "      (9): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (mean_layer): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "    (logvar_layer): Sequential(\n",
       "      (0): Linear(in_features=1024, out_features=32, bias=True)\n",
       "    )\n",
       "    (decoder): Sequential(\n",
       "      (0): Linear(in_features=32, out_features=1024, bias=True)\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Unflatten(dim=1, unflattened_size=(256, 2, 2))\n",
       "      (3): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (10): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (transform): Resize(size=16, interpolation=bilinear, max_size=None, antialias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a488b33",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52f1d9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
